{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mevaluate\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_dataset, ClassLabel, Features, Sequence, Value\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer, pipeline\n",
      "File \u001B[0;32m~/DataspellProjects/nlp-project-ae/venv/lib/python3.11/site-packages/evaluate/__init__.py:29\u001B[0m\n\u001B[1;32m     25\u001B[0m SCRIPTS_VERSION \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmain\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m version\u001B[38;5;241m.\u001B[39mparse(__version__)\u001B[38;5;241m.\u001B[39mis_devrelease \u001B[38;5;28;01melse\u001B[39;00m __version__\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m version\n\u001B[0;32m---> 29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mevaluation_suite\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m EvaluationSuite\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mevaluator\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     31\u001B[0m     AutomaticSpeechRecognitionEvaluator,\n\u001B[1;32m     32\u001B[0m     Evaluator,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     41\u001B[0m     evaluator,\n\u001B[1;32m     42\u001B[0m )\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mhub\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m push_to_hub\n",
      "File \u001B[0;32m~/DataspellProjects/nlp-project-ae/venv/lib/python3.11/site-packages/evaluate/evaluation_suite/__init__.py:10\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dataset, DownloadMode, load_dataset\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversion\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Version\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mevaluator\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m evaluator\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mloading\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m evaluation_module_factory\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfile_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DownloadConfig\n",
      "File \u001B[0;32m~/DataspellProjects/nlp-project-ae/venv/lib/python3.11/site-packages/evaluate/evaluator/__init__.py:17\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2022 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# See the License for the specific language governing permissions and\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 17\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpipelines\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SUPPORTED_TASKS \u001B[38;5;28;01mas\u001B[39;00m SUPPORTED_PIPELINE_TASKS\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpipelines\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TASK_ALIASES\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpipelines\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m check_task \u001B[38;5;28;01mas\u001B[39;00m check_pipeline_task\n",
      "File \u001B[0;32m~/DataspellProjects/nlp-project-ae/venv/lib/python3.11/site-packages/transformers/pipelines/__init__.py:63\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_extraction\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FeatureExtractionPipeline\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfill_mask\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FillMaskPipeline\n\u001B[0;32m---> 63\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage_classification\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ImageClassificationPipeline\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage_segmentation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ImageSegmentationPipeline\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage_to_text\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ImageToTextPipeline\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1178\u001B[0m, in \u001B[0;36m_find_and_load\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1149\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:690\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[0;34m(spec)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap_external>:936\u001B[0m, in \u001B[0;36mexec_module\u001B[0;34m(self, module)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap_external>:1032\u001B[0m, in \u001B[0;36mget_code\u001B[0;34m(self, fullname)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap_external>:1131\u001B[0m, in \u001B[0;36mget_data\u001B[0;34m(self, path)\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import evaluate\n",
    "from datasets import load_dataset, ClassLabel, Features, Sequence, Value\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer, pipeline\n",
    "from huggingface_hub import notebook_login"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T22:11:25.579625Z",
     "start_time": "2023-08-07T22:11:15.993229Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#notebook_login()\n",
    "# access token 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"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T22:11:25.604167Z",
     "start_time": "2023-08-07T22:11:25.590135Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RES_PATH = os.path.abspath(\"../resources/data/\") + \"/\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-07T22:11:25.591102Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_dataset_dict(dataset_name, dataset_path, limit=None):\n",
    "    train_file = dataset_path + dataset_name + \"/train.jsonl\"\n",
    "    test_file = dataset_path + dataset_name + \"/test.jsonl\"\n",
    "    dev_file = dataset_path + dataset_name + \"/dev.jsonl\"\n",
    "\n",
    "    return load_dataset(\"json\", data_files={\"train\":train_file, \"validation\":dev_file, \"test\":test_file},\n",
    "                        features=Features({\n",
    "                            \"id\": Value(dtype=\"string\", id=None),\n",
    "                            \"label\": Sequence(ClassLabel(num_classes=3, names=[\"B\", \"I\", \"O\"]), length=-1, id=None),\n",
    "                            \"sentence\": Sequence(Value(dtype=\"string\", id=None), length=-1, id=None),\n",
    "                        })).rename_column(\"label\", \"labels\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-07T22:11:25.599127Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#laptop_dataset_dict = load_dataset_dict(\"test\", RES_PATH)\n",
    "laptop_dataset_dict = load_dataset_dict(\"laptop\", RES_PATH)\n",
    "#laptop_dataset_dict.push_to_hub(\"laptop-reviews\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T22:11:25.647161Z",
     "start_time": "2023-08-07T22:11:25.604519Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "laptop_dataset_dict[\"train\"][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-07T22:11:25.608560Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#label_list = laptop_dataset_dict[\"train\"].features[\"label\"].feature.names # [\"B\", \"I\", \"O\"]\n",
    "label_list = [\"B\", \"I\", \"O\"]\n",
    "label_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-07T22:11:25.614190Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bert_auto_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-07T22:11:25.619147Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "example = laptop_dataset_dict[\"train\"][0]\n",
    "tokenized_input = bert_auto_tokenizer(example[\"sentence\"], is_split_into_words=True, padding=True, truncation=True)\n",
    "tokens = bert_auto_tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-07T22:11:25.623544Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(batch_data):\n",
    "    tokenized_inputs = bert_auto_tokenizer(batch_data[\"sentence\"], is_split_into_words=True, padding=True, truncation=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(batch_data[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100 -> ignored by PT\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-07T22:11:25.628627Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenized_laptop_dataset_dict = laptop_dataset_dict.map(tokenize_and_align_labels, batched=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-07T22:11:25.634147Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenized_laptop_dataset_dict[\"train\"].features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-07T22:11:25.638857Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=bert_auto_tokenizer, padding=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-07T22:11:25.642462Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf_metrics = evaluate.load(\"seqeval\")\n",
    "#accuracy_metric = evaluate.load(\"accuracy\")\n",
    "#precision_metric = evaluate.load(\"precision\")\n",
    "#recall_metric = evaluate.load(\"recall\")\n",
    "#f1_metric = evaluate.load(\"f1\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T22:11:25.687488Z",
     "start_time": "2023-08-07T22:11:25.647386Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = [label_list[i] for i in example[\"labels\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-07T22:11:25.652202Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    # true_labels = [label2id[item] for item in true_labels]\n",
    "    # true_predictions = [label2id[item] for item in true_predictions]\n",
    "\n",
    "    results = clf_metrics.compute(predictions=true_predictions, references=true_labels, zero_division=np.nan)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-07T22:11:25.655972Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"B\",\n",
    "    1: \"I\",\n",
    "    2: \"O\",\n",
    "}\n",
    "label2id = {\n",
    "    \"B\": 0,\n",
    "    \"I\": 1,\n",
    "    \"O\": 2\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-07T22:11:25.661001Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bert_token_classificator = AutoModelForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3, id2label=id2label, label2id=label2id)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-07T22:11:25.666477Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"aspect_extraction_laptop_reviews\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    #save_steps=100.0,\n",
    "    #save_total_limit=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-07T22:11:25.669221Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=bert_token_classificator,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_laptop_dataset_dict[\"train\"],\n",
    "    eval_dataset=tokenized_laptop_dataset_dict[\"validation\"],\n",
    "    tokenizer=bert_auto_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-07T22:11:25.671507Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-07T22:11:25.673634Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-07T22:11:25.675779Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-07T22:11:25.678700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.predict(tokenized_laptop_dataset_dict[\"test\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-07T22:11:25.682437Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = \"I Did not enjoy the new Apple operating system. I wish I could go back to the previous one!\"\n",
    "classifier = pipeline(\"ner\", model=\"jannikseus/aspect_extraction_laptop_reviews\")\n",
    "classifier(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-07T22:11:25.685961Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inf_tokenizer = AutoTokenizer.from_pretrained(\"jannikseus/aspect_extraction_laptop_reviews\")\n",
    "inputs = inf_tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "inf_model = AutoModelForTokenClassification.from_pretrained(\"jannikseus/aspect_extraction_laptop_reviews\")\n",
    "with torch.no_grad():\n",
    "    logits = inf_model(**inputs).logits\n",
    "\n",
    "inf_predictions = torch.argmax(logits, dim=2)\n",
    "inf_predicted_token_class = [inf_model.config.id2label[t.item()] for t in inf_predictions[0]]\n",
    "inf_predicted_token_class"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T22:11:25.689761Z",
     "start_time": "2023-08-07T22:11:25.688141Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-07T22:11:25.690140Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-07T22:11:25.692486Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
