{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# imports\n",
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from datasets import Features\n",
    "from datasets import Sequence\n",
    "from datasets import ClassLabel\n",
    "from datasets import Value\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, BertConfig, AutoModelForSequenceClassification, \\\n",
    "    DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T20:55:16.679039Z",
     "start_time": "2023-08-06T20:55:09.595650Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "#notebook_login()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T20:55:16.736008Z",
     "start_time": "2023-08-06T20:55:16.689760Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "RES_PATH = os.path.abspath(\"../resources/data/\") + \"/\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T20:55:16.737015Z",
     "start_time": "2023-08-06T20:55:16.723152Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def load_dataset_dict(dataset_name, dataset_path, limit=None):\n",
    "    train_file = dataset_path + dataset_name + \"/train.json\"\n",
    "    test_file = dataset_path + dataset_name + \"/test.json\"\n",
    "    dev_file = dataset_path + dataset_name + \"/dev.json\"\n",
    "\n",
    "    return load_dataset(\"json\", data_files={\"train\":train_file, \"validation\":dev_file, \"test\":test_file},\n",
    "                        features=Features({\n",
    "                            \"id\": Value(dtype=\"string\", id=None),\n",
    "                            \"label\": Sequence(ClassLabel(num_classes=3, names=[\"B\", \"I\", \"O\"]), length=-1, id=None),\n",
    "                            \"sentence\": Sequence(Value(dtype=\"string\", id=None), length=-1, id=None),\n",
    "                        })).rename_column(\"label\", \"labels\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T20:55:16.761736Z",
     "start_time": "2023-08-06T20:55:16.748941Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "laptop_dataset_dict = load_dataset_dict(\"test\", RES_PATH)\n",
    "#laptop_dataset_dict = load_dataset_dict(\"laptop\", RES_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T20:55:17.495860Z",
     "start_time": "2023-08-06T20:55:16.771079Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "{'id': '0',\n 'labels': [0, 2, 2, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 2],\n 'sentence': ['Keyboard',\n  'is',\n  'great',\n  'but',\n  'primary',\n  'and',\n  'secondary',\n  'control',\n  'buttons',\n  'could',\n  'be',\n  'more',\n  'durable',\n  '.']}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laptop_dataset_dict[\"train\"][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T20:55:17.515421Z",
     "start_time": "2023-08-06T20:55:17.502658Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "['B', 'I', 'O']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label_list = laptop_dataset_dict[\"train\"].features[\"label\"].feature.names # [\"B\", \"I\", \"O\"]\n",
    "label_list = [\"B\", \"I\", \"O\"]\n",
    "label_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T20:55:17.570419Z",
     "start_time": "2023-08-06T20:55:17.516312Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "bert_auto_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T20:55:17.946723Z",
     "start_time": "2023-08-06T20:55:17.530906Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "['[CLS]',\n 'keyboard',\n 'is',\n 'great',\n 'but',\n 'primary',\n 'and',\n 'secondary',\n 'control',\n 'buttons',\n 'could',\n 'be',\n 'more',\n 'durable',\n '.',\n '[SEP]']"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = laptop_dataset_dict[\"train\"][0]\n",
    "tokenized_input = bert_auto_tokenizer(example[\"sentence\"], is_split_into_words=True, padding=True, truncation=True)\n",
    "tokens = bert_auto_tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T20:55:17.974370Z",
     "start_time": "2023-08-06T20:55:17.952836Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(batch_data):\n",
    "    tokenized_inputs = bert_auto_tokenizer(batch_data[\"sentence\"], is_split_into_words=True, padding=True, truncation=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(batch_data[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100 -> ignored by PT\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T20:55:18.006074Z",
     "start_time": "2023-08-06T20:55:17.979028Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "tokenized_laptop_dataset_dict = laptop_dataset_dict.map(tokenize_and_align_labels, batched=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T20:55:18.063932Z",
     "start_time": "2023-08-06T20:55:17.999064Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "{'id': Value(dtype='string', id=None),\n 'labels': Sequence(feature=ClassLabel(names=['B', 'I', 'O'], id=None), length=-1, id=None),\n 'sentence': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_laptop_dataset_dict[\"train\"].features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T20:55:18.065088Z",
     "start_time": "2023-08-06T20:55:18.028834Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=bert_auto_tokenizer, padding=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T20:55:18.065436Z",
     "start_time": "2023-08-06T20:55:18.041296Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T20:55:21.735537Z",
     "start_time": "2023-08-06T20:55:18.052961Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "labels = [label_list[i] for i in example[\"labels\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T20:55:21.752481Z",
     "start_time": "2023-08-06T20:55:21.738542Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = []\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        for (p, l) in zip(prediction, label):\n",
    "            if l != -100:\n",
    "                true_predictions.append(label_list[p])\n",
    "\n",
    "    true_labels = []\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        for (p, l) in zip(prediction, label):\n",
    "            if l != -100:\n",
    "                true_labels.append(label_list[l])\n",
    "\n",
    "    true_labels = [label2id[item] for item in true_labels]\n",
    "    true_predictions = [label2id[item] for item in true_predictions]\n",
    "\n",
    "    results = {}\n",
    "    results.update(accuracy_metric.compute(predictions=true_predictions, references=true_labels))\n",
    "    results.update(precision_metric.compute(predictions=true_predictions, references=true_labels, average=\"micro\"))\n",
    "    results.update(recall_metric.compute(predictions=true_predictions, references=true_labels, average=\"micro\"))\n",
    "    results.update(f1_metric.compute(predictions=true_predictions, references=true_labels, average=\"micro\"))\n",
    "\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T20:55:21.768306Z",
     "start_time": "2023-08-06T20:55:21.753792Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"B\",\n",
    "    1: \"I\",\n",
    "    2: \"O\",\n",
    "}\n",
    "label2id = {\n",
    "    \"B\": 0,\n",
    "    \"I\": 1,\n",
    "    \"O\": 2\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T20:55:21.789339Z",
     "start_time": "2023-08-06T20:55:21.762532Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_token_classificator = AutoModelForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3, id2label=id2label, label2id=label2id)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T20:55:24.965105Z",
     "start_time": "2023-08-06T20:55:21.771373Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"aspect_extraction_laptop_reviews\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    #save_steps=100.0,\n",
    "    #save_total_limit=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T20:55:25.041177Z",
     "start_time": "2023-08-06T20:55:24.967275Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/jannikseus/aspect_extraction_laptop_reviews into local empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=bert_token_classificator,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_laptop_dataset_dict[\"train\"],\n",
    "    eval_dataset=tokenized_laptop_dataset_dict[\"test\"],\n",
    "    tokenizer=bert_auto_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T20:55:30.191020Z",
     "start_time": "2023-08-06T20:55:25.006751Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jannik/DataspellProjects/nlp-project-ae/venv/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/2 : < :, Epoch 0.50/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=2, training_loss=1.017417311668396, metrics={'train_runtime': 36.5895, 'train_samples_per_second': 0.82, 'train_steps_per_second': 0.055, 'total_flos': 283256959500.0, 'train_loss': 1.017417311668396, 'epoch': 1.0})"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T20:56:07.012790Z",
     "start_time": "2023-08-06T20:55:30.386710Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Upload file pytorch_model.bin:   0%|          | 1.00/253M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e115947f3df647b9a892e6cacecdebe5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/jannikseus/aspect_extraction_laptop_reviews\n",
      "   2f615ce..169b9f9  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T20:58:14.429482Z",
     "start_time": "2023-08-06T20:56:07.020552Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "{'eval_loss': 0.8763446807861328,\n 'eval_accuracy': 0.8134328358208955,\n 'eval_precision': 0.8134328358208955,\n 'eval_recall': 0.8134328358208955,\n 'eval_f1': 0.8134328358208955,\n 'eval_runtime': 0.6853,\n 'eval_samples_per_second': 11.674,\n 'eval_steps_per_second': 1.459,\n 'epoch': 1.0}"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T20:58:15.192057Z",
     "start_time": "2023-08-06T20:58:14.467215Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "PredictionOutput(predictions=array([[[-1.59245759e-01, -1.41889662e-01,  1.41037703e-02],\n        [-1.87129647e-01, -1.41412169e-01,  3.88076067e-01],\n        [-2.60223389e-01, -2.33298510e-01,  6.85155749e-01],\n        [-3.32794726e-01, -9.41025987e-02,  4.24694598e-01],\n        [-1.52201429e-01, -4.07698154e-01,  2.64618754e-01],\n        [-1.82764560e-01, -3.50165427e-01,  1.98252276e-01],\n        [-3.36912513e-01, -1.74226105e-01,  2.43389666e-01],\n        [-2.74318337e-01, -2.56763607e-01,  5.15795827e-01],\n        [-2.51258790e-01, -3.94350529e-01,  2.15758026e-01],\n        [-1.68532491e-01, -3.06511074e-01,  3.58344793e-01],\n        [-1.97740957e-01, -7.90176839e-02,  5.69956601e-01],\n        [-1.47146851e-01, -1.66778803e-01,  6.04994416e-01],\n        [-2.67290771e-01, -2.36941904e-01,  3.11656833e-01],\n        [-4.98436183e-01, -1.33061662e-01,  3.92339081e-01],\n        [-2.05842763e-01, -2.47040123e-01, -1.43875405e-02],\n        [ 2.94050723e-02, -2.51022339e-01,  5.15675068e-01],\n        [-2.88148314e-01, -1.81034029e-01,  3.04413497e-01],\n        [-3.21952581e-01, -2.50288188e-01,  3.62835228e-01],\n        [ 2.32862949e-01, -1.49263129e-01,  7.37169921e-01],\n        [ 2.78378755e-01, -3.04638982e-01,  7.02198505e-01],\n        [-6.61287233e-02, -1.49383307e-01, -5.43848202e-02],\n        [-6.00531325e-02, -1.49021149e-01, -4.28487957e-02],\n        [-1.56098619e-01, -1.97887018e-01,  3.85829620e-03],\n        [-1.03516579e-02, -1.42448127e-01, -3.58812585e-02],\n        [-5.26556447e-02, -1.34918720e-01, -2.39106901e-02],\n        [-1.80875719e-01, -8.25878531e-02,  9.43577141e-02],\n        [-2.38946602e-01, -1.37620136e-01,  2.15261251e-01],\n        [-1.84573472e-01, -1.07170410e-01,  1.21098697e-01],\n        [-1.71801776e-01, -1.07423507e-01,  1.08678192e-01],\n        [-2.20694765e-01, -1.03944816e-01,  1.39960214e-01],\n        [-3.40154558e-01, -1.19390994e-01,  1.76107734e-01],\n        [-3.14876288e-01, -1.81624919e-01,  1.62352353e-01],\n        [-1.76842973e-01, -1.89295501e-01,  9.40281004e-02],\n        [-3.64938319e-01, -4.56337593e-02,  2.22880334e-01],\n        [-1.87018514e-01, -1.90673426e-01,  1.33094683e-01],\n        [-3.71470273e-01, -1.40015393e-01,  1.14933923e-01],\n        [-3.75992358e-01, -2.52331048e-01,  2.22067833e-01],\n        [-1.83881611e-01, -2.29062289e-01,  2.11702198e-01],\n        [-3.71434659e-01, -3.02487493e-01,  1.27881438e-01],\n        [-2.20099241e-01, -1.00435063e-01,  9.46544707e-02],\n        [-2.70562083e-01, -9.20796692e-02,  9.20690894e-02]],\n\n       [[-2.53584325e-01, -1.51122853e-01, -1.14903145e-01],\n        [-1.99540153e-01,  9.07009467e-04,  1.71991289e-01],\n        [ 8.41562599e-02, -3.40095818e-01,  1.97320729e-01],\n        [-2.51757056e-02, -1.79323077e-01,  2.12170795e-01],\n        [-2.21454620e-01, -3.37040633e-01,  2.71323860e-01],\n        [-2.91893095e-01, -2.96702534e-01,  3.99774313e-01],\n        [-2.68505454e-01, -3.61028671e-01,  1.86262444e-01],\n        [-3.43296528e-01, -2.75574118e-01,  1.83878779e-01],\n        [-3.15164849e-02, -4.63738680e-01,  4.15545970e-01],\n        [-7.09685758e-02, -3.14112484e-01,  1.88140497e-01],\n        [-1.96612284e-01, -2.71921337e-01,  2.10664757e-02],\n        [-1.34172350e-01,  5.57760522e-03,  4.83766049e-02],\n        [ 1.18817516e-01, -5.32585979e-01,  1.56505406e-01],\n        [-6.62756041e-02, -1.94515347e-01,  1.69624567e-01],\n        [-2.87475497e-01, -4.47307765e-01,  2.97496676e-01],\n        [-1.19997866e-01, -2.03063816e-01, -2.69223526e-02],\n        [-7.74833113e-02, -3.38322401e-01, -5.13039604e-02],\n        [-8.25399831e-02, -1.13570631e-01,  4.67722490e-03],\n        [-1.48864761e-01, -1.09349683e-01,  3.05005133e-01],\n        [-5.32241464e-01,  2.16809168e-01,  8.02666917e-02],\n        [-2.65961438e-01, -8.16060603e-02,  3.59820873e-01],\n        [-2.70261616e-01, -1.84169531e-01,  3.18488419e-01],\n        [-3.74063253e-01, -1.97538957e-01,  2.18430221e-01],\n        [-4.82479900e-01,  2.19867378e-01,  7.76636600e-02],\n        [-3.29524487e-01, -1.02677196e-02,  9.60013270e-02],\n        [-2.42253944e-01, -2.30258867e-01,  1.37993559e-01],\n        [ 9.72198993e-02, -3.20337296e-01,  7.97558784e-01],\n        [-2.77375191e-01, -1.57437727e-01,  2.72167295e-01],\n        [-9.51169059e-02, -3.85100394e-01,  1.03434339e-01],\n        [ 2.02903941e-01, -5.34074940e-02,  6.98620796e-01],\n        [ 1.60389423e-01, -2.70348907e-01,  6.13201737e-01],\n        [-1.65323913e-01, -2.02002153e-01,  9.93485525e-02],\n        [-1.39846772e-01, -1.97070569e-01,  7.14414492e-02],\n        [-2.12530017e-01, -2.13143677e-01,  9.87868309e-02],\n        [-1.64482415e-01, -1.38414159e-01,  1.17798984e-01],\n        [-2.01419741e-01, -2.58357048e-01,  1.25415862e-01],\n        [-1.98469430e-01, -2.68162340e-01,  9.41748098e-02],\n        [-1.92184836e-01, -1.57578945e-01,  1.15900777e-01],\n        [-1.80954143e-01, -2.04034135e-01,  1.77260041e-01],\n        [-2.74876446e-01, -1.07465193e-01,  1.48133650e-01],\n        [-1.96008921e-01, -1.51710123e-01,  6.12226799e-02]],\n\n       [[-1.92710102e-01, -1.26785904e-01, -1.77901119e-01],\n        [ 2.24604994e-01, -3.62330042e-02,  8.93223062e-02],\n        [-4.18602154e-02, -3.47138405e-01,  4.34160531e-01],\n        [-2.97671318e-01,  3.47752459e-02,  4.35850650e-01],\n        [-3.89013439e-01, -3.83065641e-01,  1.46545097e-01],\n        [-4.98381257e-03, -4.45949495e-01,  1.23114593e-01],\n        [ 3.38288754e-01, -1.11417741e-01,  4.90031600e-01],\n        [-9.68361795e-02, -2.33979493e-01,  5.98210335e-01],\n        [-2.25422427e-01, -3.02601695e-01,  3.31954151e-01],\n        [-2.43532196e-01, -4.22465265e-01,  1.20407730e-01],\n        [-1.83617488e-01, -4.71170992e-04,  7.47127458e-02],\n        [ 3.40809524e-01, -6.36250153e-02,  4.20298457e-01],\n        [-1.45887345e-01, -3.39506596e-01,  4.88392919e-01],\n        [-2.24693358e-01, -4.77123767e-01,  1.62628785e-01],\n        [-1.73054680e-01, -3.28863323e-01,  1.48502484e-01],\n        [ 3.48388195e-01, -1.24596588e-01,  4.03885096e-01],\n        [-1.34619817e-01, -3.73080254e-01,  4.38181221e-01],\n        [-1.99970245e-01, -4.43061471e-01,  8.90626162e-02],\n        [-3.67801972e-02, -1.00196131e-01,  1.12569913e-01],\n        [-6.30255640e-02, -3.13046247e-01,  6.67754412e-02],\n        [ 2.77973473e-01, -1.76422566e-01,  3.83693725e-01],\n        [-1.25959054e-01, -4.17120457e-01,  4.16730940e-01],\n        [-1.12071522e-01, -1.72767684e-01,  2.17494503e-01],\n        [-1.66599423e-01, -1.07129529e-01,  1.50013238e-01],\n        [-1.99937582e-01, -2.22379223e-01,  7.18195438e-02],\n        [-2.28885651e-01, -9.71157700e-02,  2.73149818e-01],\n        [-2.66110718e-01, -4.48839366e-01,  4.42521721e-02],\n        [ 4.75874171e-04, -3.32254648e-01, -1.09509267e-02],\n        [ 2.79150382e-02, -1.87607005e-01,  7.58075267e-02],\n        [ 2.54820704e-01, -1.25454426e-01,  3.53000998e-01],\n        [-1.72065988e-01, -4.25970852e-01,  4.50030148e-01],\n        [ 1.28502667e-01, -3.79020184e-01,  1.13012053e-01],\n        [-1.47766769e-01, -3.17049265e-01,  2.67780691e-01],\n        [-8.96840319e-02, -2.14175209e-01,  5.77278212e-02],\n        [ 3.50023717e-01, -3.01124811e-01,  7.45133758e-01],\n        [-1.98210105e-01, -1.05156973e-01,  3.01382661e-01],\n        [-2.14248464e-01, -3.60121548e-01,  4.95475531e-01],\n        [-2.89358288e-01, -3.60418677e-01,  1.82712138e-01],\n        [-7.06226081e-02, -2.47412056e-01,  2.00587183e-01],\n        [ 3.26519191e-01, -1.38830408e-01,  6.87934160e-01],\n        [ 1.01617143e-01, -3.85161370e-01,  4.55963612e-01]]],\n      dtype=float32), label_ids=array([[-100,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n           2,    2,    2,    2,    2,    2,    2,    2, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100],\n       [-100,    2,    2, -100,    2,    2,    0,    1,    2,    2,    2,\n           2, -100, -100,    2,    2,    0,    2,    2,    2,    2,    2,\n           2,    2,    2,    2,    2, -100,    2,    2, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100],\n       [-100,    2,    2,    2,    2,    0,    2,    2,    2,    2,    0,\n           2,    2,    2,    2,    2,    2,    2,    2, -100,    2,    2,\n           2,    2,    2,    2,    2,    2,    0,    2,    2,    2,    2,\n           0,    2,    2,    2,    2,    2,    2, -100]]), metrics={'test_loss': 0.8423812985420227, 'test_accuracy': 0.8518518518518519, 'test_precision': 0.8518518518518519, 'test_recall': 0.8518518518518519, 'test_f1': 0.8518518518518519, 'test_runtime': 0.2139, 'test_samples_per_second': 14.024, 'test_steps_per_second': 4.675})"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(tokenized_laptop_dataset_dict[\"validation\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T20:58:15.437619Z",
     "start_time": "2023-08-06T20:58:15.187755Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T20:58:15.448238Z",
     "start_time": "2023-08-06T20:58:15.440127Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T20:58:15.451430Z",
     "start_time": "2023-08-06T20:58:15.445944Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T20:58:15.453854Z",
     "start_time": "2023-08-06T20:58:15.451319Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T20:58:15.527317Z",
     "start_time": "2023-08-06T20:58:15.494938Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
