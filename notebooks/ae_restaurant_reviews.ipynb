{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# imports\n",
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from datasets import Features\n",
    "from datasets import Sequence\n",
    "from datasets import ClassLabel\n",
    "from datasets import Value\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, BertConfig, AutoModelForSequenceClassification, \\\n",
    "    DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T21:00:11.283338Z",
     "start_time": "2023-08-06T21:00:04.513020Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "#notebook_login()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T21:00:11.303605Z",
     "start_time": "2023-08-06T21:00:11.290880Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "RES_PATH = os.path.abspath(\"../resources/data/\") + \"/\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T21:00:11.333261Z",
     "start_time": "2023-08-06T21:00:11.305612Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def load_dataset_dict(dataset_name, dataset_path, limit=None):\n",
    "    train_file = dataset_path + dataset_name + \"/train.json\"\n",
    "    test_file = dataset_path + dataset_name + \"/test.json\"\n",
    "    dev_file = dataset_path + dataset_name + \"/dev.json\"\n",
    "\n",
    "    return load_dataset(\"json\", data_files={\"train\":train_file, \"validation\":dev_file, \"test\":test_file},\n",
    "                        features=Features({\n",
    "                            \"id\": Value(dtype=\"string\", id=None),\n",
    "                            \"label\": Sequence(ClassLabel(num_classes=3, names=[\"B\", \"I\", \"O\"]), length=-1, id=None),\n",
    "                            \"sentence\": Sequence(Value(dtype=\"string\", id=None), length=-1, id=None),\n",
    "                        })).rename_column(\"label\", \"labels\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T21:00:11.333918Z",
     "start_time": "2023-08-06T21:00:11.320434Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "rest_dataset_dict = load_dataset_dict(\"test\", RES_PATH)\n",
    "#rest_dataset_dict = load_dataset_dict(\"rest\", RES_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T21:00:12.321505Z",
     "start_time": "2023-08-06T21:00:11.339422Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "{'id': '0',\n 'labels': [0, 2, 2, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 2],\n 'sentence': ['Keyboard',\n  'is',\n  'great',\n  'but',\n  'primary',\n  'and',\n  'secondary',\n  'control',\n  'buttons',\n  'could',\n  'be',\n  'more',\n  'durable',\n  '.']}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_dataset_dict[\"train\"][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T21:00:12.341389Z",
     "start_time": "2023-08-06T21:00:12.332739Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "['B', 'I', 'O']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label_list = rest_dataset_dict[\"train\"].features[\"label\"].feature.names # [\"B\", \"I\", \"O\"]\n",
    "label_list = [\"B\", \"I\", \"O\"]\n",
    "label_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T21:00:12.372588Z",
     "start_time": "2023-08-06T21:00:12.342359Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "bert_auto_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T21:00:12.667676Z",
     "start_time": "2023-08-06T21:00:12.353878Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "['[CLS]',\n 'keyboard',\n 'is',\n 'great',\n 'but',\n 'primary',\n 'and',\n 'secondary',\n 'control',\n 'buttons',\n 'could',\n 'be',\n 'more',\n 'durable',\n '.',\n '[SEP]']"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = rest_dataset_dict[\"train\"][0]\n",
    "tokenized_input_rest = bert_auto_tokenizer(example[\"sentence\"], is_split_into_words=True, padding=True, truncation=True)\n",
    "tokens_rest = bert_auto_tokenizer.convert_ids_to_tokens(tokenized_input_rest[\"input_ids\"])\n",
    "tokens_rest"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T21:00:12.685088Z",
     "start_time": "2023-08-06T21:00:12.670752Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(batch_data):\n",
    "    tokenized_inputs = bert_auto_tokenizer(batch_data[\"sentence\"], is_split_into_words=True, padding=True, truncation=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(batch_data[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100 -> ignored by PT\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T21:00:12.721113Z",
     "start_time": "2023-08-06T21:00:12.688244Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "tokenized_rest_dataset_dict = rest_dataset_dict.map(tokenize_and_align_labels, batched=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T21:00:12.740321Z",
     "start_time": "2023-08-06T21:00:12.707022Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "{'id': Value(dtype='string', id=None),\n 'labels': Sequence(feature=ClassLabel(names=['B', 'I', 'O'], id=None), length=-1, id=None),\n 'sentence': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_rest_dataset_dict[\"train\"].features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T21:00:12.789094Z",
     "start_time": "2023-08-06T21:00:12.737040Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=bert_auto_tokenizer, padding=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T21:00:12.798062Z",
     "start_time": "2023-08-06T21:00:12.745287Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T21:00:16.828248Z",
     "start_time": "2023-08-06T21:00:12.762160Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "labels = [label_list[i] for i in example[\"labels\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T21:00:16.841095Z",
     "start_time": "2023-08-06T21:00:16.835756Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = []\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        for (p, l) in zip(prediction, label):\n",
    "            if l != -100:\n",
    "                true_predictions.append(label_list[p])\n",
    "\n",
    "    true_labels = []\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        for (p, l) in zip(prediction, label):\n",
    "            if l != -100:\n",
    "                true_labels.append(label_list[l])\n",
    "\n",
    "    true_labels = [label2id[item] for item in true_labels]\n",
    "    true_predictions = [label2id[item] for item in true_predictions]\n",
    "\n",
    "    results = {}\n",
    "    results.update(accuracy_metric.compute(predictions=true_predictions, references=true_labels))\n",
    "    results.update(precision_metric.compute(predictions=true_predictions, references=true_labels, average=\"micro\"))\n",
    "    results.update(recall_metric.compute(predictions=true_predictions, references=true_labels, average=\"micro\"))\n",
    "    results.update(f1_metric.compute(predictions=true_predictions, references=true_labels, average=\"micro\"))\n",
    "\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T21:00:16.863853Z",
     "start_time": "2023-08-06T21:00:16.850240Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"B\",\n",
    "    1: \"I\",\n",
    "    2: \"O\",\n",
    "}\n",
    "label2id = {\n",
    "    \"B\": 0,\n",
    "    \"I\": 1,\n",
    "    \"O\": 2\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T21:00:16.867026Z",
     "start_time": "2023-08-06T21:00:16.858276Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_token_classificator = AutoModelForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3, id2label=id2label, label2id=label2id)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T21:00:18.704962Z",
     "start_time": "2023-08-06T21:00:16.868008Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"aspect_extraction_restaurant_reviews\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    #save_steps=100.0,\n",
    "    #save_total_limit=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T21:00:18.746627Z",
     "start_time": "2023-08-06T21:00:18.707938Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jannik/DataspellProjects/nlp-project-ae/notebooks/aspect_extraction_restaurant_reviews is already a clone of https://huggingface.co/jannikseus/aspect_extraction_restaurant_reviews. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=bert_token_classificator,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_rest_dataset_dict[\"train\"],\n",
    "    eval_dataset=tokenized_rest_dataset_dict[\"test\"],\n",
    "    tokenizer=bert_auto_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T21:00:21.598916Z",
     "start_time": "2023-08-06T21:00:18.741791Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jannik/DataspellProjects/nlp-project-ae/venv/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/2 : < :, Epoch 0.50/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=2, training_loss=1.1182729005813599, metrics={'train_runtime': 24.778, 'train_samples_per_second': 1.211, 'train_steps_per_second': 0.081, 'total_flos': 283256959500.0, 'train_loss': 1.1182729005813599, 'epoch': 1.0})"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T21:00:46.579681Z",
     "start_time": "2023-08-06T21:00:21.783619Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Upload file pytorch_model.bin:   0%|          | 1.00/253M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3ea76423a00d477fa1f61b9736ee422c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/jannikseus/aspect_extraction_restaurant_reviews\n",
      "   54d5b4a..1cda4cb  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T21:02:50.712605Z",
     "start_time": "2023-08-06T21:00:46.583216Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "{'eval_loss': 0.9864752292633057,\n 'eval_accuracy': 0.5597014925373134,\n 'eval_precision': 0.5597014925373134,\n 'eval_recall': 0.5597014925373134,\n 'eval_f1': 0.5597014925373134,\n 'eval_runtime': 0.5613,\n 'eval_samples_per_second': 14.253,\n 'eval_steps_per_second': 1.782,\n 'epoch': 1.0}"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T21:02:51.299149Z",
     "start_time": "2023-08-06T21:02:50.721214Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "PredictionOutput(predictions=array([[[ 2.60803550e-01, -1.20568916e-01,  1.01086825e-01],\n        [ 6.14641048e-02, -6.55389205e-02,  4.87749055e-02],\n        [ 2.67148077e-01, -6.06393814e-03,  2.71650434e-01],\n        [ 6.88832104e-02, -2.08010897e-01,  2.77577132e-01],\n        [ 1.65450573e-01, -3.58482659e-01,  2.26256847e-01],\n        [ 7.63909519e-02, -2.54156619e-01,  1.70909703e-01],\n        [ 3.29094902e-02, -1.20776355e-01,  3.96264732e-01],\n        [ 3.01099688e-01, -1.11149773e-01,  3.32548320e-01],\n        [ 2.89300323e-01, -1.34115368e-01,  2.61822402e-01],\n        [ 2.22279117e-01, -1.50743514e-01,  1.23772010e-01],\n        [ 1.51342154e-01, -3.14011812e-01,  2.73642033e-01],\n        [-7.39142299e-04, -4.92152758e-02,  9.84793156e-02],\n        [-3.85688134e-02,  6.42010868e-02,  1.10937528e-01],\n        [ 1.64174020e-01,  1.68363675e-01, -2.67640874e-02],\n        [ 1.95947886e-01,  2.27761894e-01,  1.84585929e-01],\n        [ 4.50445950e-01, -6.62415773e-02,  2.88740218e-01],\n        [ 1.96610123e-01,  1.21549144e-01,  7.14480579e-02],\n        [-2.51816064e-01, -4.62690145e-02,  1.61213636e-01],\n        [-2.33466148e-01,  3.61354649e-02,  2.60456115e-01],\n        [-1.07702352e-01, -8.02596360e-02,  4.67124522e-01],\n        [-2.96793208e-02, -2.79456884e-01,  9.55176577e-02],\n        [-1.18655972e-02, -2.81843305e-01,  7.89658576e-02],\n        [-1.57639906e-02, -2.41917193e-01,  1.56718642e-01],\n        [-9.51430947e-03, -3.04243326e-01,  4.71373200e-02],\n        [-2.46897042e-02, -2.94875085e-01,  6.15171492e-02],\n        [-9.04724374e-03, -1.75214514e-01,  8.43674541e-02],\n        [ 6.81882650e-02, -2.11060643e-01,  1.34945735e-01],\n        [ 7.55661726e-03, -1.87612325e-01,  1.31559268e-01],\n        [-3.90703306e-02, -1.99892297e-01,  1.20516434e-01],\n        [ 5.53011522e-03, -2.01697230e-01,  2.32523710e-01],\n        [-4.46051359e-04, -1.43419266e-01,  2.13025659e-01],\n        [ 1.35262832e-02, -1.40434742e-01,  2.11699963e-01],\n        [-3.89898345e-02, -2.10859418e-01,  1.59672037e-01],\n        [ 4.46101874e-02, -2.39348948e-01,  2.36451119e-01],\n        [-1.94150172e-02, -1.87111244e-01,  2.48306394e-01],\n        [ 7.20335022e-02, -1.58291698e-01,  2.27622718e-01],\n        [ 8.58160555e-02, -1.08597316e-01,  1.75590217e-01],\n        [-1.04138628e-02, -2.20495969e-01,  1.45749122e-01],\n        [ 1.58725411e-01, -1.84521198e-01,  1.35407239e-01],\n        [-5.64122349e-02, -1.07791558e-01,  1.03576548e-01],\n        [-3.63711938e-02, -5.40224686e-02,  1.24886535e-01]],\n\n       [[ 2.81208456e-01, -1.78465933e-01, -8.48274082e-02],\n        [ 2.30379134e-01, -3.99100482e-01, -1.26734421e-01],\n        [ 1.98903993e-01, -5.17085791e-01, -2.44784877e-02],\n        [ 1.30234003e-01, -5.33135474e-01,  1.31082535e-02],\n        [ 1.18547671e-01, -4.95559275e-01,  9.40037519e-02],\n        [ 3.31654459e-01, -5.57910621e-01, -5.81255406e-02],\n        [ 1.79510638e-02, -5.91205716e-01,  7.96503872e-02],\n        [ 3.43633667e-02, -5.66972375e-01, -6.22708574e-02],\n        [ 1.48801938e-01, -3.28908563e-01, -2.85097510e-02],\n        [ 6.86070025e-02, -2.83704430e-01,  9.28497612e-02],\n        [ 8.95099267e-02, -3.57776076e-01,  2.02251241e-01],\n        [ 1.16201445e-01, -2.38323346e-01, -4.65798527e-02],\n        [-2.76908204e-02, -4.88909781e-01,  1.32337045e-02],\n        [-9.03992951e-02, -2.75637597e-01,  8.38758796e-02],\n        [ 9.74294245e-02, -4.41129357e-01, -5.74959330e-02],\n        [ 1.74726203e-01, -3.34325999e-01, -1.23122513e-01],\n        [ 9.93425548e-02, -3.18572819e-01, -1.04640186e-01],\n        [ 1.83275297e-01, -2.92976379e-01,  8.69837180e-02],\n        [ 3.40072095e-01, -6.01652265e-01,  8.19244757e-02],\n        [ 1.88360497e-01,  8.79090056e-02,  5.02716720e-01],\n        [ 3.55903506e-01, -5.34709871e-01,  7.25891441e-03],\n        [ 1.54582590e-01, -1.98391050e-01,  2.19754204e-02],\n        [ 2.20505595e-02, -1.75100833e-01,  3.45065415e-01],\n        [ 2.96025753e-01, -3.28443795e-02,  4.45448399e-01],\n        [ 2.89913893e-01, -3.76669139e-01,  4.68087643e-02],\n        [ 9.03877616e-03, -4.46499288e-01, -1.04980901e-01],\n        [ 1.70702592e-01, -9.84463543e-02,  2.78087705e-01],\n        [ 2.95867503e-01, -2.39614040e-01, -4.63775396e-02],\n        [-3.54645438e-02, -3.75581205e-01,  9.23632234e-02],\n        [-1.08396143e-01, -1.65332496e-01,  1.87988907e-01],\n        [-5.62491715e-02, -3.03964436e-01,  3.09697062e-01],\n        [-4.51157652e-02, -3.37906867e-01,  1.31696105e-01],\n        [-1.12026215e-01, -3.58596265e-01,  4.95026857e-02],\n        [-8.95150751e-03, -3.11207712e-01,  6.83874711e-02],\n        [-1.32945210e-01, -2.61883765e-01,  8.10364038e-02],\n        [-8.56226087e-02, -3.40705663e-01,  9.84380767e-02],\n        [-7.08700716e-02, -3.05306464e-01,  6.68851659e-02],\n        [ 5.78841642e-02, -2.71997362e-01,  1.75167993e-02],\n        [-6.84483498e-02, -1.99146330e-01,  4.14640456e-02],\n        [-9.39190909e-02, -2.15094298e-01,  1.30841851e-01],\n        [-1.42243102e-01, -2.11658448e-01,  1.39448836e-01]],\n\n       [[ 3.57416362e-01, -2.26297796e-01,  3.46589237e-02],\n        [ 3.33116472e-01, -3.23982954e-01, -1.29490599e-01],\n        [ 2.81397939e-01, -3.82521659e-01, -5.91125563e-02],\n        [ 3.69629443e-01, -2.05672443e-01,  2.29914665e-01],\n        [ 8.34067538e-02, -3.07817966e-01,  1.85452908e-01],\n        [ 1.89245105e-01, -6.54683530e-01,  8.16858560e-02],\n        [ 3.48422408e-01, -5.44967115e-01,  3.08317482e-01],\n        [ 2.50322402e-01, -3.17970812e-01,  2.04774737e-01],\n        [ 1.09792814e-01, -2.87988454e-01,  2.61639446e-01],\n        [-7.27149844e-02, -1.96306676e-01,  2.15239704e-01],\n        [ 1.29599035e-01, -2.89079607e-01,  1.11480623e-01],\n        [ 2.69491017e-01, -5.43986380e-01,  3.82480800e-01],\n        [ 1.71872437e-01, -3.93871963e-01,  1.26506895e-01],\n        [ 1.67762101e-01, -2.79156923e-01,  2.29050189e-01],\n        [-3.70266959e-02, -3.15940380e-01,  6.46149963e-02],\n        [ 3.01677257e-01, -5.56598008e-01,  3.66775185e-01],\n        [ 1.94470957e-01, -3.39144766e-01,  1.56085879e-01],\n        [ 1.83985174e-01, -2.44344011e-01,  2.01352239e-01],\n        [ 9.53942016e-02, -1.54686511e-01, -8.89814645e-02],\n        [ 8.87778401e-02, -7.50665516e-02, -7.03957677e-03],\n        [ 3.32798123e-01, -4.79832053e-01,  3.80098701e-01],\n        [ 2.47115389e-01, -3.68116617e-01,  1.84346810e-01],\n        [ 2.07792893e-01, -3.15786839e-01,  3.07069048e-02],\n        [ 2.33349532e-01, -1.29728407e-01,  2.51110643e-01],\n        [ 4.63822752e-01, -1.04569830e-01,  3.44104886e-01],\n        [ 2.38816649e-01, -5.89321554e-02,  1.47365183e-01],\n        [ 3.77489001e-01, -1.95210010e-01,  4.99593198e-01],\n        [ 2.02971682e-01, -2.68263519e-01,  1.74460858e-01],\n        [ 1.37790486e-01, -2.18853042e-01,  6.06330819e-02],\n        [ 3.19069833e-01, -4.30912852e-01,  3.76510233e-01],\n        [ 2.44245470e-01, -2.52950370e-01,  1.89150825e-01],\n        [ 2.46011436e-01, -2.12558135e-02,  2.35469997e-01],\n        [ 1.91020250e-01, -4.01963629e-02,  1.47197843e-02],\n        [ 1.63518816e-01, -2.39763290e-01,  8.14510882e-02],\n        [-9.52487811e-02, -9.28796157e-02,  3.83294582e-01],\n        [ 2.61552930e-01, -1.42071635e-01,  1.13621332e-01],\n        [ 1.87624753e-01, -2.04869241e-01,  1.78794846e-01],\n        [ 1.15396075e-01, -1.13887705e-01,  2.30003834e-01],\n        [-2.45256238e-02, -2.70982683e-01,  9.15158689e-02],\n        [-1.91412643e-01, -4.45596911e-02,  2.42964640e-01],\n        [ 4.81656551e-01, -3.58200133e-01,  2.92407870e-01]]],\n      dtype=float32), label_ids=array([[-100,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n           2,    2,    2,    2,    2,    2,    2,    2, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100],\n       [-100,    2,    2, -100,    2,    2,    0,    1,    2,    2,    2,\n           2, -100, -100,    2,    2,    0,    2,    2,    2,    2,    2,\n           2,    2,    2,    2,    2, -100,    2,    2, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100],\n       [-100,    2,    2,    2,    2,    0,    2,    2,    2,    2,    0,\n           2,    2,    2,    2,    2,    2,    2,    2, -100,    2,    2,\n           2,    2,    2,    2,    2,    2,    0,    2,    2,    2,    2,\n           0,    2,    2,    2,    2,    2,    2, -100]]), metrics={'test_loss': 0.9925646185874939, 'test_accuracy': 0.49382716049382713, 'test_precision': 0.49382716049382713, 'test_recall': 0.49382716049382713, 'test_f1': 0.49382716049382713, 'test_runtime': 0.2118, 'test_samples_per_second': 14.165, 'test_steps_per_second': 4.722})"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(tokenized_rest_dataset_dict[\"validation\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T21:02:52.092858Z",
     "start_time": "2023-08-06T21:02:51.865984Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
