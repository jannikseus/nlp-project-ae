{
 "cells": [
  {
   "cell_type": "raw",
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import datasets\n",
    "# imports\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from numpy.core import overrides\n",
    "from datasets import load_dataset, Dataset\n",
    "from datasets import Features\n",
    "from datasets import ClassLabel\n",
    "from datasets import Value\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, BertConfig, AutoModelForSequenceClassification, \\\n",
    "    TrainingArguments, Trainer\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T11:08:32.807989Z",
     "start_time": "2023-07-02T11:08:32.785969100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "RES_PATH = os.path.abspath('../resources/data/') + '/'\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T11:08:32.843020900Z",
     "start_time": "2023-07-02T11:08:32.797980Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  label   \n0            [B, O, O, O, O, O, O, B, I, O, O, O, O, O]  \\\n1            [O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n2                  [O, O, O, O, O, O, O, O, O, O, O, O]   \n3      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n4     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n...                                                 ...   \n2890  [O, O, O, O, O, O, O, O, B, I, O, O, O, O, O, ...   \n2891  [O, B, I, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n2892  [O, O, O, O, O, O, O, O, B, O, O, O, O, O, O, ...   \n2893            [O, O, O, O, O, O, O, O, O, O, O, O, O]   \n2894  [O, O, O, O, O, O, O, O, O, O, B, O, O, O, O, ...   \n\n                                               sentence  \n0     [Keyboard, is, great, but, primary, and, secon...  \n1     [I, bought, this, laptop, about, a, month, ago...  \n2     [I, am, however, pleased, that, it, is, still,...  \n3     [I, went, to, my, local, Best, Buy, looking, f...  \n4     [The, Apple, MC371LL/, A, 2.4Ghz, 15.4-, inch,...  \n...                                                 ...  \n2890  [After, talking, it, over, with, the, very, kn...  \n2891  [If, internet, connectivity, is, important, I,...  \n2892  [My, friend, just, had, to, replace, his, enti...  \n2893  [I, work, with, kids, and, they, love, making,...  \n2894  [I, am, forever, changed, and, will, no, longe...  \n\n[2895 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[B, O, O, O, O, O, O, B, I, O, O, O, O, O]</td>\n      <td>[Keyboard, is, great, but, primary, and, secon...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n      <td>[I, bought, this, laptop, about, a, month, ago...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O]</td>\n      <td>[I, am, however, pleased, that, it, is, still,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n      <td>[I, went, to, my, local, Best, Buy, looking, f...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n      <td>[The, Apple, MC371LL/, A, 2.4Ghz, 15.4-, inch,...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2890</th>\n      <td>[O, O, O, O, O, O, O, O, B, I, O, O, O, O, O, ...</td>\n      <td>[After, talking, it, over, with, the, very, kn...</td>\n    </tr>\n    <tr>\n      <th>2891</th>\n      <td>[O, B, I, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n      <td>[If, internet, connectivity, is, important, I,...</td>\n    </tr>\n    <tr>\n      <th>2892</th>\n      <td>[O, O, O, O, O, O, O, O, B, O, O, O, O, O, O, ...</td>\n      <td>[My, friend, just, had, to, replace, his, enti...</td>\n    </tr>\n    <tr>\n      <th>2893</th>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n      <td>[I, work, with, kids, and, they, love, making,...</td>\n    </tr>\n    <tr>\n      <th>2894</th>\n      <td>[O, O, O, O, O, O, O, O, O, O, B, O, O, O, O, ...</td>\n      <td>[I, am, forever, changed, and, will, no, longe...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2895 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data into Series (retrieving train, test and optimization sets)\n",
    "with open(RES_PATH + 'laptop/train.json', 'r', encoding='utf-8') as file_train_laptop:\n",
    "    raw_train_laptop = pd.read_json(file_train_laptop, encoding='utf-8', orient='index').fillna(\"\")\n",
    "\n",
    "with open(RES_PATH + 'laptop/test.json', 'r', encoding='utf-8') as file_test_laptop:\n",
    "    raw_test_laptop = pd.read_json(file_test_laptop, encoding='utf-8', orient='index').fillna(\"\")\n",
    "\n",
    "with open(RES_PATH + 'laptop/dev.json', 'r', encoding='utf-8') as file_dev_laptop:\n",
    "    raw_dev_laptop = pd.read_json(file_dev_laptop, encoding='utf-8', orient='index').fillna(\"\")\n",
    "\n",
    "with open(RES_PATH + 'rest/train.json', 'r', encoding='utf-8') as file_train_res:\n",
    "    raw_train_res = pd.read_json(file_train_res, encoding='utf-8', orient='index').fillna(\"\")\n",
    "\n",
    "with open(RES_PATH + 'rest/test.json', 'r', encoding='utf-8') as file_test_res:\n",
    "    raw_test_res = pd.read_json(file_test_res, encoding='utf-8', orient='index').fillna(\"\")\n",
    "\n",
    "with open(RES_PATH + 'rest/dev.json', 'r', encoding='utf-8') as file_dev_res:\n",
    "    raw_dev_res = pd.read_json(file_dev_res, encoding='utf-8', orient='index').fillna(\"\")\n",
    "\n",
    "raw_train_laptop\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T11:08:32.880054400Z",
     "start_time": "2023-07-02T11:08:32.812993600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "\n",
    "# [The, Apple, MC371LL/, A, 2.4Ghz, 15.4-, inch, MacBook, Pro, Notebook, is, a, horrible, waste, of, money, .]\n",
    "# -> [The, Apple MC371LL/A 2.4Ghz 15.4-inch MacBook Pro Notebook, is, a, horrible, waste, of, money, .]\n",
    "\n",
    "# [Did, n't, expect, to, repair, it, at, once, !]\n",
    "# -> [Didn't, expect, to, repair, it, at, once, !]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T11:08:32.894067100Z",
     "start_time": "2023-07-02T11:08:32.877051900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# data reduction:\n",
    "# convert data to lowercase, correct spelling, remove/handle stopwords, remove/handle punctuation, remove/handle numbers, remove/handle special characters\n",
    "# detect ambiguous words and re-label them correctly"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T11:08:32.939108Z",
     "start_time": "2023-07-02T11:08:32.894067100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# data transformation:\n",
    "# unnecessary for this project, data already in the correct format: string and integer values are suitable for the model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T11:08:32.959126200Z",
     "start_time": "2023-07-02T11:08:32.909080900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# data cleaning:\n",
    "# remove too short/long sentences, remove duplicate sentences, remove empty sentences, remove sentences with only stopwords, remove sentences with only punctuation, remove sentences with only numbers, remove sentences with only special characters"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T11:08:32.959126200Z",
     "start_time": "2023-07-02T11:08:32.939108Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# data integration:\n",
    "# unnecessary for this project, only one datasource present"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T11:08:32.960127Z",
     "start_time": "2023-07-02T11:08:32.941109800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# modeling\n",
    "# https://www.geeksforgeeks.org/hidden-markov-model-in-machine-learning/\n",
    "\n",
    "# modeling (Co-Pilot suggestions LOL):\n",
    "# model selection: Naive Bayes, SVM, Decision Tree, Random Forest, Logistic Regression, Neural Network, RNN, LSTM, BERT\n",
    "# model training: 70/10/20 split, 5-fold cross-validation\n",
    "# model optimization: hyperparameter tuning, cross-validation, grid search, random search\n",
    "# model testing:\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T11:08:32.976141800Z",
     "start_time": "2023-07-02T11:08:32.958125400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "ds_train_laptop = Dataset.from_pandas(raw_train_laptop)\n",
    "ds_test_laptop = Dataset.from_pandas(raw_test_laptop)\n",
    "ds_dev_laptop = Dataset.from_pandas(raw_dev_laptop)\n",
    "\n",
    "ds_train_res = Dataset.from_pandas(raw_train_res)\n",
    "ds_test_res = Dataset.from_pandas(raw_test_res)\n",
    "ds_dev_res = Dataset.from_pandas(raw_dev_res)\n",
    "\n",
    "laptop_dataset_dict = datasets.DatasetDict({\n",
    "    \"train\": ds_train_laptop,\n",
    "    \"test\": ds_test_laptop,\n",
    "    \"dev\": ds_dev_laptop\n",
    "})\n",
    "\n",
    "res_dataset_dict = datasets.DatasetDict({\n",
    "    \"train\": ds_train_res,\n",
    "    \"test\": ds_test_res,\n",
    "    \"dev\": ds_dev_res\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T11:08:33.005168300Z",
     "start_time": "2023-07-02T11:08:32.974139800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T11:08:33.880835200Z",
     "start_time": "2023-07-02T11:08:33.005168300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/2895 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "853b02d5c6e74a67afc75b2ffd261c61"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/800 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b209cc511d8e4f4fbacc3f65920d4018"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/150 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b0e6f995f904a479d4256fa16f2e099"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/1850 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf84189b96254a83ac054f5317dd1d56"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/676 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "16afe82a302443999aae34e08149efa3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/150 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "23159aabf86546009a7e45e74353c52a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define function to tokenize data (sentence column) with BERT tokenizer\n",
    "def tokenize_data(dataset_dict):\n",
    "    return tokenizer(\n",
    "        dataset_dict['sentence'],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        is_split_into_words=True)\n",
    "\n",
    "# tokenize data with defined function\n",
    "laptop_dataset_dict_enc = laptop_dataset_dict.map(tokenize_data, batched=True)\n",
    "res_dataset_dict_enc = res_dataset_dict.map(tokenize_data, batched=True)\n",
    "\n",
    "# set format to torch\n",
    "laptop_dataset_dict_enc.set_format(\"torch\")\n",
    "res_dataset_dict_enc.set_format(\"torch\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T11:08:35.257649900Z",
     "start_time": "2023-07-02T11:08:33.865821Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['label', 'sentence', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 150\n})"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laptop_dataset_dict_enc[\"dev\"]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T11:08:35.272808300Z",
     "start_time": "2023-07-02T11:08:35.258650200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "clf_metrics = evaluate.combine([\"accuracy\", 'f1', \"precision\", \"recall\"])\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return clf_metrics.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"bert-base-uncased_auto-seq-model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=laptop_dataset_dict[\"train\"],\n",
    "    eval_dataset=laptop_dataset_dict[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T11:08:37.786880700Z",
     "start_time": "2023-07-02T11:08:35.275810800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.args.device\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T11:08:37.803891600Z",
     "start_time": "2023-07-02T11:08:37.786880700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jannik\\programming_projects\\nlp-project-ae\\venv\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['label']",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[34], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\programming_projects\\nlp-project-ae\\venv\\lib\\site-packages\\transformers\\trainer.py:1645\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   1640\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_wrapped \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\n\u001B[0;32m   1642\u001B[0m inner_training_loop \u001B[38;5;241m=\u001B[39m find_executable_batch_size(\n\u001B[0;32m   1643\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inner_training_loop, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_batch_size, args\u001B[38;5;241m.\u001B[39mauto_find_batch_size\n\u001B[0;32m   1644\u001B[0m )\n\u001B[1;32m-> 1645\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1646\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1647\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1648\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1649\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1650\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\programming_projects\\nlp-project-ae\\venv\\lib\\site-packages\\transformers\\trainer.py:1916\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   1913\u001B[0m     rng_to_sync \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   1915\u001B[0m step \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 1916\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step, inputs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(epoch_iterator):\n\u001B[0;32m   1917\u001B[0m     total_batched_samples \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1918\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m rng_to_sync:\n",
      "File \u001B[1;32m~\\programming_projects\\nlp-project-ae\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    631\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    632\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 633\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    634\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    635\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    636\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\programming_projects\\nlp-project-ae\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    675\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    676\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 677\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    678\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    679\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\programming_projects\\nlp-project-ae\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\programming_projects\\nlp-project-ae\\venv\\lib\\site-packages\\transformers\\data\\data_collator.py:249\u001B[0m, in \u001B[0;36mDataCollatorWithPadding.__call__\u001B[1;34m(self, features)\u001B[0m\n\u001B[0;32m    248\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, features: List[Dict[\u001B[38;5;28mstr\u001B[39m, Any]]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, Any]:\n\u001B[1;32m--> 249\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    250\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    251\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    252\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    253\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    256\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m batch:\n\u001B[0;32m    257\u001B[0m         batch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\programming_projects\\nlp-project-ae\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2977\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.pad\u001B[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001B[0m\n\u001B[0;32m   2975\u001B[0m \u001B[38;5;66;03m# The model's main input name, usually `input_ids`, has be passed for padding\u001B[39;00m\n\u001B[0;32m   2976\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_input_names[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m encoded_inputs:\n\u001B[1;32m-> 2977\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   2978\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou should supply an encoding or a list of encodings to this method \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2979\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthat includes \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_input_names[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, but you provided \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(encoded_inputs\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2980\u001B[0m     )\n\u001B[0;32m   2982\u001B[0m required_input \u001B[38;5;241m=\u001B[39m encoded_inputs[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_input_names[\u001B[38;5;241m0\u001B[39m]]\n\u001B[0;32m   2984\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m required_input \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(required_input, Sized) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(required_input) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m):\n",
      "\u001B[1;31mValueError\u001B[0m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['label']"
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T11:08:38.011073800Z",
     "start_time": "2023-07-02T11:08:37.801889700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.evaluate()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T11:08:38.011073800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.evaluate(laptop_dataset_dict[\"dev\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model evaluation with metrics precision, recall, accuracy, f1-score and confusion matrix\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
