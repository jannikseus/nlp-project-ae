{
 "cells": [
  {
   "cell_type": "raw",
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "import datasets\n",
    "# imports\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from numpy.core import overrides\n",
    "from datasets import load_dataset, Dataset\n",
    "from datasets import Features\n",
    "from datasets import ClassLabel\n",
    "from datasets import Value\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, BertConfig, AutoModelForSequenceClassification, \\\n",
    "    TrainingArguments, Trainer\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T15:37:53.247959600Z",
     "start_time": "2023-07-07T15:37:53.208924800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "RES_PATH = os.path.abspath('../resources/data/') + '/'\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T15:37:53.280989700Z",
     "start_time": "2023-07-07T15:37:53.219934300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  label   \n0            [B, O, O, O, O, O, O, B, I, O, O, O, O, O]  \\\n1            [O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n2                  [O, O, O, O, O, O, O, O, O, O, O, O]   \n3      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n4     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n...                                                 ...   \n2890  [O, O, O, O, O, O, O, O, B, I, O, O, O, O, O, ...   \n2891  [O, B, I, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n2892  [O, O, O, O, O, O, O, O, B, O, O, O, O, O, O, ...   \n2893            [O, O, O, O, O, O, O, O, O, O, O, O, O]   \n2894  [O, O, O, O, O, O, O, O, O, O, B, O, O, O, O, ...   \n\n                                               sentence  \n0     [Keyboard, is, great, but, primary, and, secon...  \n1     [I, bought, this, laptop, about, a, month, ago...  \n2     [I, am, however, pleased, that, it, is, still,...  \n3     [I, went, to, my, local, Best, Buy, looking, f...  \n4     [The, Apple, MC371LL/, A, 2.4Ghz, 15.4-, inch,...  \n...                                                 ...  \n2890  [After, talking, it, over, with, the, very, kn...  \n2891  [If, internet, connectivity, is, important, I,...  \n2892  [My, friend, just, had, to, replace, his, enti...  \n2893  [I, work, with, kids, and, they, love, making,...  \n2894  [I, am, forever, changed, and, will, no, longe...  \n\n[2895 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[B, O, O, O, O, O, O, B, I, O, O, O, O, O]</td>\n      <td>[Keyboard, is, great, but, primary, and, secon...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n      <td>[I, bought, this, laptop, about, a, month, ago...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O]</td>\n      <td>[I, am, however, pleased, that, it, is, still,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n      <td>[I, went, to, my, local, Best, Buy, looking, f...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n      <td>[The, Apple, MC371LL/, A, 2.4Ghz, 15.4-, inch,...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2890</th>\n      <td>[O, O, O, O, O, O, O, O, B, I, O, O, O, O, O, ...</td>\n      <td>[After, talking, it, over, with, the, very, kn...</td>\n    </tr>\n    <tr>\n      <th>2891</th>\n      <td>[O, B, I, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n      <td>[If, internet, connectivity, is, important, I,...</td>\n    </tr>\n    <tr>\n      <th>2892</th>\n      <td>[O, O, O, O, O, O, O, O, B, O, O, O, O, O, O, ...</td>\n      <td>[My, friend, just, had, to, replace, his, enti...</td>\n    </tr>\n    <tr>\n      <th>2893</th>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n      <td>[I, work, with, kids, and, they, love, making,...</td>\n    </tr>\n    <tr>\n      <th>2894</th>\n      <td>[O, O, O, O, O, O, O, O, O, O, B, O, O, O, O, ...</td>\n      <td>[I, am, forever, changed, and, will, no, longe...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2895 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data into Series (retrieving train, test and optimization sets)\n",
    "with open(RES_PATH + 'laptop/train.json', 'r', encoding='utf-8') as file_train_laptop:\n",
    "    raw_train_laptop = pd.read_json(file_train_laptop, encoding='utf-8', orient='index').fillna(\"\")\n",
    "\n",
    "with open(RES_PATH + 'laptop/test.json', 'r', encoding='utf-8') as file_test_laptop:\n",
    "    raw_test_laptop = pd.read_json(file_test_laptop, encoding='utf-8', orient='index').fillna(\"\")\n",
    "\n",
    "with open(RES_PATH + 'laptop/dev.json', 'r', encoding='utf-8') as file_dev_laptop:\n",
    "    raw_dev_laptop = pd.read_json(file_dev_laptop, encoding='utf-8', orient='index').fillna(\"\")\n",
    "\n",
    "with open(RES_PATH + 'rest/train.json', 'r', encoding='utf-8') as file_train_res:\n",
    "    raw_train_res = pd.read_json(file_train_res, encoding='utf-8', orient='index').fillna(\"\")\n",
    "\n",
    "with open(RES_PATH + 'rest/test.json', 'r', encoding='utf-8') as file_test_res:\n",
    "    raw_test_res = pd.read_json(file_test_res, encoding='utf-8', orient='index').fillna(\"\")\n",
    "\n",
    "with open(RES_PATH + 'rest/dev.json', 'r', encoding='utf-8') as file_dev_res:\n",
    "    raw_dev_res = pd.read_json(file_dev_res, encoding='utf-8', orient='index').fillna(\"\")\n",
    "\n",
    "raw_train_laptop\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T15:37:53.285995Z",
     "start_time": "2023-07-07T15:37:53.234948Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "\n",
    "# [The, Apple, MC371LL/, A, 2.4Ghz, 15.4-, inch, MacBook, Pro, Notebook, is, a, horrible, waste, of, money, .]\n",
    "# -> [The, Apple MC371LL/A 2.4Ghz 15.4-inch MacBook Pro Notebook, is, a, horrible, waste, of, money, .]\n",
    "\n",
    "# [Did, n't, expect, to, repair, it, at, once, !]\n",
    "# -> [Didn't, expect, to, repair, it, at, once, !]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T15:37:53.300006800Z",
     "start_time": "2023-07-07T15:37:53.283992300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "# data reduction:\n",
    "# convert data to lowercase, correct spelling, remove/handle stopwords, remove/handle punctuation, remove/handle numbers, remove/handle special characters\n",
    "# detect ambiguous words and re-label them correctly"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T15:37:53.323027900Z",
     "start_time": "2023-07-07T15:37:53.299006Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "# data transformation:\n",
    "# unnecessary for this project, data already in the correct format: string and integer values are suitable for the model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T15:37:53.357057400Z",
     "start_time": "2023-07-07T15:37:53.315020800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "# data cleaning:\n",
    "# remove too short/long sentences, remove duplicate sentences, remove empty sentences, remove sentences with only stopwords, remove sentences with only punctuation, remove sentences with only numbers, remove sentences with only special characters"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T15:37:53.369068600Z",
     "start_time": "2023-07-07T15:37:53.331034700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "# data integration:\n",
    "# unnecessary for this project, only one datasource present"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T15:37:53.370069600Z",
     "start_time": "2023-07-07T15:37:53.348049400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "# modeling\n",
    "# https://www.geeksforgeeks.org/hidden-markov-model-in-machine-learning/\n",
    "\n",
    "# modeling (Co-Pilot suggestions LOL):\n",
    "# model selection: SVM, Decision Tree, Random Forest, Logistic Regression, Neural Network, RNN, LSTM, BERT\n",
    "# model training: 70/10/20 split, 5-fold cross-validation\n",
    "# model optimization: hyperparameter tuning, cross-validation, #grid search, random search\n",
    "# model testing:\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T15:37:53.377076100Z",
     "start_time": "2023-07-07T15:37:53.363063200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "class_label = ClassLabel(num_classes=3, names=['B', 'I', 'O'])\n",
    "# map class labels: 'B'->0, 'I'->1, 'O'->2\n",
    "\n",
    "ds_train_laptop = Dataset.from_pandas(raw_train_laptop)\n",
    "ds_test_laptop = Dataset.from_pandas(raw_test_laptop)\n",
    "ds_dev_laptop = Dataset.from_pandas(raw_dev_laptop)\n",
    "\n",
    "ds_train_res = Dataset.from_pandas(raw_train_res)\n",
    "ds_test_res = Dataset.from_pandas(raw_test_res)\n",
    "ds_dev_res = Dataset.from_pandas(raw_dev_res)\n",
    "\n",
    "laptop_dataset_dict = datasets.DatasetDict({\n",
    "    \"train\": ds_train_laptop,\n",
    "    \"test\": ds_test_laptop,\n",
    "    \"dev\": ds_dev_laptop\n",
    "})\n",
    "\n",
    "res_dataset_dict = datasets.DatasetDict({\n",
    "    \"train\": ds_train_res,\n",
    "    \"test\": ds_test_res,\n",
    "    \"dev\": ds_dev_res\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T15:37:53.408104100Z",
     "start_time": "2023-07-07T15:37:53.378076800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model_bert = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)\n",
    "# ..."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T15:37:54.062194100Z",
     "start_time": "2023-07-07T15:37:53.409104700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/2895 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "69a9d452ab5148da8d863182835b0887"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/800 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00d0be4669254836ba74c2123b9f39c2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/150 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d6751f3c08b040148dc12f08f3c0ce73"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/1850 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d01f13d14664d50af9fd89f8606ecab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/676 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3823845a21f04f5793d5a18b355e46e7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/150 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "001e8c6eadbb4741b4509a57f9b358db"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define function to tokenize data (sentence column) with BERT tokenizer\n",
    "def dataset_labels_to_int(dataset_dict):\n",
    "    dataset_dict['label'] = [class_label.str2int(label) for label in dataset_dict['label']]\n",
    "    return dataset_dict\n",
    "\n",
    "# convert class labels to int with defined function\n",
    "laptop_dataset_dict_enc = laptop_dataset_dict.map(dataset_labels_to_int, batched=True)\n",
    "res_dataset_dict_enc = res_dataset_dict.map(dataset_labels_to_int, batched=True)\n",
    "\n",
    "# set format to torch\n",
    "#laptop_dataset_dict_enc.set_format(\"torch\")\n",
    "#laptop_dataset_dict_enc.set_format(\"torch\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T15:37:54.240354100Z",
     "start_time": "2023-07-07T15:37:54.048181800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "data": {
      "text/plain": "[[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2,\n  2,\n  2,\n  2,\n  0,\n  2,\n  2,\n  2,\n  2,\n  0,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  0,\n  2,\n  2,\n  2,\n  2,\n  0,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2],\n [2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2],\n [2, 0, 1, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 0, 1, 1, 1, 2, 2, 2, 2, 2],\n [2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  0,\n  2,\n  2,\n  2,\n  2],\n [2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 0, 1, 2, 2, 2, 0, 2, 0, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 0, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  0,\n  2,\n  0,\n  2,\n  2,\n  2,\n  2,\n  2,\n  0,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  0,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 0, 2, 2, 0, 2],\n [2, 2, 2, 2, 2, 0, 2, 2],\n [2, 2, 2, 2, 2],\n [2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2,\n  0,\n  2,\n  2,\n  2,\n  2,\n  0,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  0,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 0, 2, 2, 2, 0, 2],\n [2, 2, 2, 2],\n [2, 2, 0, 1, 2, 0, 2, 2, 2, 2, 2],\n [2, 2, 0, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 0, 1, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 0, 2, 2, 0, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 0, 2, 0, 2],\n [2, 0, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 0, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2],\n [2,\n  2,\n  2,\n  2,\n  0,\n  1,\n  1,\n  1,\n  2,\n  2,\n  0,\n  2,\n  2,\n  0,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  0,\n  1,\n  2,\n  2,\n  2,\n  0,\n  1,\n  2,\n  2,\n  2,\n  0,\n  1,\n  1,\n  2,\n  2,\n  0,\n  1,\n  2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2],\n [2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  0,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  0,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  0,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2],\n [2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2],\n [2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  0,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2],\n [2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 1, 1, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2],\n [2, 2, 2, 0, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 0, 2, 2, 2, 0, 1, 2, 0, 1, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2],\n [2, 0, 2, 2, 2, 0, 2, 2, 2],\n [2, 0, 1, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2],\n [2, 2, 2, 2, 2, 2, 2, 0, 2],\n [2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  0,\n  1,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 0, 1, 2, 2, 0, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  0,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2,\n  0,\n  1,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2],\n [2, 2, 2, 2, 2],\n [2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2],\n [2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2],\n [2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  0,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 0, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  0,\n  1,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  0,\n  1,\n  2,\n  2,\n  2,\n  2,\n  2,\n  0,\n  2,\n  2,\n  2,\n  0,\n  2,\n  2,\n  2,\n  2,\n  0,\n  2],\n [2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  0,\n  1,\n  1,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2],\n [2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2,\n  2,\n  2,\n  2,\n  0,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  0,\n  1,\n  2,\n  2,\n  2,\n  2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 0, 1, 2, 2, 0, 1, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 0, 1, 1, 1, 2],\n [2, 2, 2, 2, 2, 2, 0, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2,\n  0,\n  1,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  0,\n  1,\n  1,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  0,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2],\n [2,\n  2,\n  2,\n  2,\n  0,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  0,\n  2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 0, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2],\n [2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 0, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2,\n  0,\n  1,\n  1,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2],\n [0, 2, 2, 2, 2, 2, 2, 2, 0, 1, 2, 2, 2, 0, 2],\n [2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  0,\n  1,\n  1,\n  1,\n  2,\n  0,\n  1,\n  1,\n  1,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  0,\n  2,\n  0,\n  2,\n  2,\n  2,\n  2,\n  2],\n [2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n [2, 2, 2, 2, 2],\n [2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2],\n [2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2]]"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laptop_dataset_dict_enc[\"dev\"][\"label\"]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T15:37:54.302407400Z",
     "start_time": "2023-07-07T15:37:54.240354100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "clf_metrics = evaluate.combine([\"accuracy\", 'f1', \"precision\", \"recall\"])\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return clf_metrics.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"bert-base-uncased_auto-seq-model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_bert,\n",
    "    args=args,\n",
    "    train_dataset=laptop_dataset_dict_enc[\"train\"],\n",
    "    eval_dataset=laptop_dataset_dict_enc[\"test\"],\n",
    "    #tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T15:37:57.217726400Z",
     "start_time": "2023-07-07T15:37:54.272383100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.args.device\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T15:37:57.232738200Z",
     "start_time": "2023-07-07T15:37:57.218727300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jannik\\programming_projects\\nlp-project-ae\\venv\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 10 at dim 1 (got 13)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[100], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\programming_projects\\nlp-project-ae\\venv\\lib\\site-packages\\transformers\\trainer.py:1645\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   1640\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_wrapped \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\n\u001B[0;32m   1642\u001B[0m inner_training_loop \u001B[38;5;241m=\u001B[39m find_executable_batch_size(\n\u001B[0;32m   1643\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inner_training_loop, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_batch_size, args\u001B[38;5;241m.\u001B[39mauto_find_batch_size\n\u001B[0;32m   1644\u001B[0m )\n\u001B[1;32m-> 1645\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1646\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1647\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1648\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1649\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1650\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\programming_projects\\nlp-project-ae\\venv\\lib\\site-packages\\transformers\\trainer.py:1916\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   1913\u001B[0m     rng_to_sync \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   1915\u001B[0m step \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 1916\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step, inputs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(epoch_iterator):\n\u001B[0;32m   1917\u001B[0m     total_batched_samples \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1918\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m rng_to_sync:\n",
      "File \u001B[1;32m~\\programming_projects\\nlp-project-ae\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    631\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    632\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 633\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    634\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    635\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    636\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\programming_projects\\nlp-project-ae\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    675\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    676\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 677\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    678\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    679\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\programming_projects\\nlp-project-ae\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\programming_projects\\nlp-project-ae\\venv\\lib\\site-packages\\transformers\\data\\data_collator.py:70\u001B[0m, in \u001B[0;36mdefault_data_collator\u001B[1;34m(features, return_tensors)\u001B[0m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;66;03m# In this function we'll make the assumption that all `features` in the batch\u001B[39;00m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;66;03m# have the same attributes.\u001B[39;00m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;66;03m# So we will look at the first element as a proxy for what attributes exist\u001B[39;00m\n\u001B[0;32m     67\u001B[0m \u001B[38;5;66;03m# on the whole batch.\u001B[39;00m\n\u001B[0;32m     69\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_tensors \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch_default_data_collator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m return_tensors \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tf_default_data_collator(features)\n",
      "File \u001B[1;32m~\\programming_projects\\nlp-project-ae\\venv\\lib\\site-packages\\transformers\\data\\data_collator.py:119\u001B[0m, in \u001B[0;36mtorch_default_data_collator\u001B[1;34m(features)\u001B[0m\n\u001B[0;32m    117\u001B[0m     label \u001B[38;5;241m=\u001B[39m first[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(first[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m], torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;28;01melse\u001B[39;00m first[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    118\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mlong \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(label, \u001B[38;5;28mint\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mfloat\n\u001B[1;32m--> 119\u001B[0m     batch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlabel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mf\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    120\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m first \u001B[38;5;129;01mand\u001B[39;00m first[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(first[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m], torch\u001B[38;5;241m.\u001B[39mTensor):\n",
      "\u001B[1;31mValueError\u001B[0m: expected sequence of length 10 at dim 1 (got 13)"
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T15:37:57.487950200Z",
     "start_time": "2023-07-07T15:37:57.233739200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.evaluate()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.evaluate(laptop_dataset_dict[\"dev\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model evaluation with metrics precision, recall, accuracy, f1-score and confusion matrix\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
